{"name":"Cluster: PostgreSQL-HA-Prod","entityType":"DatabaseCluster","observations":["Type: PostgreSQL 16 with Patroni","Architecture: 3-node cluster (1 Primary + 2 Replicas)","Region: us-east-1 (multi-AZ)","SLA: 99.99% uptime","RTO: 30 seconds (Recovery Time Objective)","RPO: 0 seconds (Synchronous replication)","Last Failover Test: 2026-01-10","Monitoring: Prometheus + Grafana + PagerDuty","[2026-01-15 10:24:13] Topology changed: New primary is pg-replica-02","[2026-01-15 10:45:00] Cluster fully recovered","Current Topology:","  PRIMARY: pg-replica-02 (10.0.2.20)","  SYNC_REPLICA: pg-primary-01 (10.0.1.10)","  ASYNC_REPLICA: pg-replica-03 (10.0.3.30)","Total Incidents This Month: 3","MTTR (Mean Time To Recovery): 42 seconds","Uptime This Month: 99.997%","[2026-01-15 02:00:49] üö® INCIDENT: Primary node pg-replica-02 unreachable","[2026-01-15 02:01:15] Failover completed: pg-replica-03 is new PRIMARY","  PRIMARY: pg-replica-03 (10.0.3.30)","  FAILED: pg-replica-02 (10.0.2.20)","Total Incidents This Month: 4","Cluster Status: ‚ö†Ô∏è DEGRADED (2 healthy nodes)"],"updatedBy":"Mai Th√†nh Duy An","updatedAt":1768442484}
{"name":"Node: pg-primary-01","entityType":"DatabaseNode","observations":["Role: PRIMARY","Status: ‚úÖ HEALTHY","IP: 10.0.1.10","Availability Zone: us-east-1a","Instance Type: r6g.4xlarge (16 vCPU, 128GB RAM)","Storage: 2TB gp3 EBS (16000 IOPS)","Connections: 450/500 (90%)","Replication Lag: N/A (is primary)","Last Health Check: 2026-01-15 09:00:00 UTC","Uptime: 45 days","[2026-01-15 10:23:45] STATUS CHANGE: HEALTHY ‚Üí FAILED (OOM kill)","[2026-01-15 10:24:13] Role changed: PRIMARY ‚Üí REBUILDING","[2026-01-15 10:45:00] Rejoined cluster as SYNC_REPLICA","[2026-01-15 11:00:00] Status: ‚úÖ HEALTHY (demoted)","Current Role: SYNC_REPLICA (was PRIMARY)","Failover Priority: 1 (can be promoted again)"]}
{"name":"Node: pg-replica-02","entityType":"DatabaseNode","observations":["Role: SYNC_REPLICA","Status: ‚úÖ HEALTHY","IP: 10.0.2.20","Availability Zone: us-east-1b","Instance Type: r6g.4xlarge","Failover Priority: 1 (First to promote)","Replication Lag: 0 bytes (synchronous)","Read Traffic: 35% of total queries","Last Health Check: 2026-01-15 09:00:00 UTC","[2026-01-15 10:23:55] Detected primary unreachable","[2026-01-15 10:24:05] Started promotion process","[2026-01-15 10:24:13] PROMOTED TO PRIMARY","[2026-01-15 10:24:13] Role changed: SYNC_REPLICA ‚Üí PRIMARY","Current Role: PRIMARY","Status: ‚úÖ HEALTHY (now primary)","Connections: 485/500 (97%) - increased after failover","[2026-01-15 02:00:49] ‚ö†Ô∏è ALERT: Health check FAILED","[2026-01-15 02:00:52] STATUS CHANGE: HEALTHY ‚Üí CRITICAL","[2026-01-15 02:00:55] Root Cause: Disk I/O timeout (EBS degraded)","[2026-01-15 02:00:55] Current Role: PRIMARY (UNRESPONSIVE)","üî¥ FAILOVER REQUIRED - Searching for backup node...","[2026-01-15 02:01:15] Demoted: PRIMARY ‚Üí FAILED","[2026-01-15 02:01:15] Failover completed to: pg-replica-03","Status: üî¥ FAILED (disk I/O)","Awaiting manual recovery..."],"updatedBy":"Mai Th√†nh Duy An","updatedAt":1768442484}
{"name":"Node: pg-replica-03","entityType":"DatabaseNode","observations":["Role: ASYNC_REPLICA","Status: ‚úÖ HEALTHY","IP: 10.0.3.30","Availability Zone: us-east-1c","Instance Type: r6g.2xlarge (cost optimization)","Failover Priority: 2 (Secondary backup)","Replication Lag: 120 bytes (~50ms)","Read Traffic: 25% of total queries","Purpose: Disaster recovery + Reporting queries","[2026-01-15 02:01:00] üü° Failover initiated - Promoting to PRIMARY","[2026-01-15 02:01:05] Acquiring leader lock from etcd...","[2026-01-15 02:01:08] Leader lock acquired","[2026-01-15 02:01:10] Role changed: ASYNC_REPLICA ‚Üí PRIMARY","[2026-01-15 02:01:12] HAProxy routing updated","[2026-01-15 02:01:15] ‚úÖ PROMOTED TO PRIMARY","Current Role: PRIMARY","Status: ‚úÖ HEALTHY (now primary)","Data Loss: ~50ms lag (~12 transactions)"],"updatedBy":"Mai Th√†nh Duy An","updatedAt":1768442484}
{"name":"Component: Patroni-Cluster","entityType":"HAComponent","observations":["Type: Cluster Manager / Consensus","Version: Patroni 3.2.1","DCS: etcd (3-node cluster)","Failover Mode: Automatic","Leader Election: Raft consensus via etcd","Watchdog: Enabled (prevents split-brain)","Failover Timeout: 30 seconds","Minimum Replicas for Failover: 1"]}
{"name":"Component: HAProxy-LB","entityType":"HAComponent","observations":["Type: Load Balancer / Connection Router","Version: HAProxy 2.8","Deployment: 2 instances (Active-Passive)","Virtual IP: 10.0.0.100","Health Check: TCP + PostgreSQL query (SELECT 1)","Routing: Writes ‚Üí Primary, Reads ‚Üí Replicas (round-robin)","Connection Pool: PgBouncer integrated"]}
{"name":"Component: etcd-cluster","entityType":"HAComponent","observations":["Type: Distributed Key-Value Store","Version: etcd 3.5.11","Nodes: 3 (quorum = 2)","Purpose: Leader election, cluster state","Data: Cluster topology, leader lock","Consensus: Raft protocol","If 2 nodes fail: Cluster becomes read-only"]}
{"name":"Alert: High-Replication-Lag","entityType":"MonitoringAlert","observations":["Trigger: Replication lag > 1MB or > 5 seconds","Severity: WARNING ‚Üí CRITICAL (if > 10MB)","Action: Page on-call DBA","Runbook: /runbooks/replication-lag.md","Auto-remediation: None (requires investigation)"]}
{"name":"Alert: Node-Unreachable","entityType":"MonitoringAlert","observations":["Trigger: Health check fails 3 consecutive times","Severity: CRITICAL","Action: Patroni initiates failover if Primary","Page: On-call DBA + SRE team","Escalation: 5 min ‚Üí Manager, 15 min ‚Üí VP Eng"]}
{"name":"Alert: Connection-Pool-Exhausted","entityType":"MonitoringAlert","observations":["Trigger: Active connections > 90% pool size","Severity: WARNING","Action: Scale read replicas or increase pool","Auto-remediation: PgBouncer pause/resume"]}
{"name":"Incident: Primary-Node-Crash-2026-01-15","entityType":"Incident","observations":["Type: Unplanned Failover","Timestamp: 2026-01-15 10:23:45 UTC","Root Cause: Out of Memory (OOM) kill on pg-primary-01","Impact: 28 seconds of write unavailability","Detection: Alert: Node-Unreachable fired at T+0s","Response Timeline:","  T+0s: Health check fails, Alert: Node-Unreachable fires","  T+3s: Patroni detects primary unreachable","  T+5s: Patroni verifies via etcd quorum","  T+8s: Leader lock released","  T+10s: pg-replica-02 starts promotion","  T+18s: pg-replica-02 becomes new PRIMARY","  T+22s: HAProxy updates routing table","  T+28s: Applications reconnect, writes resume","Failover Success: ‚úÖ YES","Data Loss: 0 bytes (synchronous replication)","Post-incident: OOM caused by runaway query, added query timeout"]}
{"name":"Incident: Network-Partition-2026-01-12","entityType":"Incident","observations":["Type: Network Split-Brain Scenario","Timestamp: 2026-01-12 03:15:00 UTC","Root Cause: AWS AZ-1a network isolation (AWS issue)","Affected Nodes: pg-primary-01 (isolated)","Detection: pg-replica-02 and pg-replica-03 lost connection to primary","Challenge: Primary still running, could accept writes","Response Timeline:","  T+0s: Network partition detected","  T+5s: Patroni watchdog kicks in on pg-primary-01","  T+8s: Primary STONITH (Shoot The Other Node In The Head)","  T+10s: Primary demotes itself (no etcd access)","  T+15s: pg-replica-02 acquires leader lock","  T+20s: pg-replica-02 promoted to PRIMARY","  T+45s: Network restored, pg-primary-01 rejoins as replica","Split-Brain Prevented: ‚úÖ YES (watchdog + STONITH)","Data Loss: 0 bytes","Lesson: Watchdog is critical for split-brain prevention"]}
{"name":"Incident: Cascading-Failure-2026-01-08","entityType":"Incident","observations":["Type: Multi-Node Failure","Timestamp: 2026-01-08 14:30:00 UTC","Root Cause: EBS volume degradation in us-east-1a and us-east-1b","Affected: pg-primary-01 AND pg-replica-02 (both sync nodes)","Impact: Complete cluster unavailability for 4 minutes","Timeline:","  T+0s: pg-primary-01 I/O errors, becomes unresponsive","  T+5s: pg-replica-02 I/O errors (same EBS issue)","  T+10s: Only pg-replica-03 (async) available","  T+15s: Patroni attempts failover to pg-replica-03","  T+20s: BLOCKED - async replica may have data loss","  T+30s: Manual intervention required (on-call DBA)","  T+120s: DBA approves async promotion (accept RPO > 0)","  T+150s: pg-replica-03 promoted with 50ms data lag","  T+240s: EBS recovered, old nodes rebuild as replicas","Data Loss: ~50ms of transactions (~12 writes)","Lesson: Sync replicas in different failure domains critical","Action: Added pg-replica-04 in us-east-1d as second sync"]}
{"name":"CaseStudy: GitHub-MySQL-2012","entityType":"CaseStudy","observations":["Company: GitHub","Date: September 2012","System: MySQL Primary-Replica Cluster","What Happened:","  - Primary MySQL server failed due to hardware issue","  - Automatic failover promoted replica","  - BUT: Replication lag of 30 seconds","  - Result: 30 seconds of data lost (commits, issues)","Impact:","  - User data loss (commits disappeared)","  - 2+ hours of incident response","  - Trust damage with developers","Root Cause:","  - Asynchronous replication without lag monitoring","  - No circuit breaker for high-lag failover","Lessons Learned:","  - Implemented synchronous replication for critical data","  - Added replication lag alerts","  - Built custom failover tool (later: Orchestrator)","  - Zero-data-loss became policy for writes","Reference: https://github.blog/2012-09-14-github-availability-this-week/"]}
{"name":"CaseStudy: AWS-Aurora-Multi-AZ-2020","entityType":"CaseStudy","observations":["Company: Amazon Web Services (Aurora)","Date: November 2020 (us-east-1 outage)","System: Aurora Multi-AZ PostgreSQL","What Happened:","  - Kinesis outage caused cascading failures","  - Internal services couldn't communicate","  - Aurora instances couldn't reach control plane","  - Automatic failover BLOCKED by control plane dependency","Impact:","  - 6+ hours of degraded service","  - Many Aurora clusters stuck in failover state","  - Read replicas couldn't promote","Why HA Failed:","  - Control plane was single point of failure","  - Failover required control plane communication","  - Local decision-making was disabled","Lessons for Memory-Graph:","  - Patroni's local watchdog is critical","  - Don't depend on remote service for failover decision","  - etcd cluster should be local to data nodes","Reference: https://aws.amazon.com/message/11201/"]}
{"name":"CaseStudy: Netflix-Cassandra-Zone-Failure-2019","entityType":"CaseStudy","observations":["Company: Netflix","Date: 2019 (multiple incidents)","System: Cassandra cluster (100+ nodes, 3 AZs)","Architecture:","  - Replication Factor = 3 (one copy per AZ)","  - Consistency Level = LOCAL_QUORUM","  - Zone-aware replica placement","What Happened:","  - Full AZ failure (us-east-1c)","  - 33% of nodes unreachable instantly","How Netflix Handled It:","  - LOCAL_QUORUM still achievable (2 of 3 AZs)","  - Reads and writes continued normally","  - No data loss (RF=3)","  - Automatic repair when AZ recovered","Key Success Factors:","  - Quorum-based consensus (no single primary)","  - Zone-aware placement (no 2 replicas in same AZ)","  - Consistent hashing (no central coordinator)","  - Chaos Engineering (they tested this regularly)","Lesson for Memory-Graph:","  - Consider leaderless replication for extreme HA","  - Test failure scenarios regularly (Chaos Monkey)","Reference: Netflix Tech Blog - Cassandra Availability Improvements"]}
{"name":"CaseStudy: Cloudflare-PostgreSQL-2023","entityType":"CaseStudy","observations":["Company: Cloudflare","Date: January 2023","System: PostgreSQL with Stolon (similar to Patroni)","What Happened:","  - Primary crashed during high load","  - Stolon initiated automatic failover","  - BUT: etcd cluster also under high load","  - etcd leader election took 45 seconds (normally 5s)","  - Total failover time: 90 seconds (target: 30s)","Impact:","  - 90 seconds of API errors","  - Queued requests caused thundering herd","  - Secondary outage during recovery","Root Cause Analysis:","  - etcd and PostgreSQL shared same network","  - Network saturation affected both","  - No resource isolation between critical components","Fix Applied:","  - Dedicated network for etcd cluster","  - Increased etcd cluster size (3 ‚Üí 5 nodes)","  - Added request queueing with exponential backoff","  - Implemented connection pooling (PgBouncer)","Lesson for Memory-Graph:","  - Isolate consensus layer from data layer","  - Monitor etcd latency as primary metric","  - Circuit breaker for connection storms","Reference: Cloudflare Blog - Incident Report January 2023"]}
{"name":"Incident: Primary-Disk-Failure-2026-01-15-02","entityType":"Incident","observations":["Type: Unplanned Failover (Disk I/O)","Timestamp: 2026-01-15 02:00:49 UTC","Failed Node: pg-replica-02 (was PRIMARY)","Root Cause: EBS volume I/O timeout","Detection Method: Memory Graph traverse('failover_backup_for')","Failover Node Found: pg-replica-03","Failover Node Status: ‚úÖ HEALTHY (verified)","Response Timeline:","  T+0s: Health check failed on pg-replica-02","  T+3s: Alert fired, Memory Graph queried","  T+6s: traverse() found pg-replica-03 as backup","  T+11s: pg-replica-03 acquires leader lock","  T+15s: Promotion complete, HAProxy updated","  T+26s: Applications reconnect","Total Failover Time: 26 seconds","Data Loss: ~12 transactions (50ms async lag)","Cluster Status: DEGRADED (2/3 nodes)","Recovery Action: Replace EBS volume on pg-replica-02"],"createdBy":"Mai Th√†nh Duy An","updatedBy":"Mai Th√†nh Duy An","createdAt":1768442496,"updatedAt":1768442496}
{"from":"Node: pg-primary-01","to":"Cluster: PostgreSQL-HA-Prod","relationType":"member_of"}
{"from":"Node: pg-replica-02","to":"Cluster: PostgreSQL-HA-Prod","relationType":"member_of"}
{"from":"Node: pg-replica-03","to":"Cluster: PostgreSQL-HA-Prod","relationType":"member_of"}
{"from":"Node: pg-replica-02","to":"Node: pg-primary-01","relationType":"replicates_from"}
{"from":"Node: pg-replica-03","to":"Node: pg-primary-01","relationType":"replicates_from"}
{"from":"Node: pg-replica-02","to":"Node: pg-primary-01","relationType":"failover_backup_for"}
{"from":"Node: pg-replica-03","to":"Node: pg-replica-02","relationType":"failover_backup_for"}
{"from":"Component: Patroni-Cluster","to":"Cluster: PostgreSQL-HA-Prod","relationType":"manages"}
{"from":"Component: HAProxy-LB","to":"Cluster: PostgreSQL-HA-Prod","relationType":"routes_to"}
{"from":"Component: etcd-cluster","to":"Component: Patroni-Cluster","relationType":"provides_consensus_for"}
{"from":"Alert: High-Replication-Lag","to":"Cluster: PostgreSQL-HA-Prod","relationType":"monitors"}
{"from":"Alert: Node-Unreachable","to":"Cluster: PostgreSQL-HA-Prod","relationType":"monitors"}
{"from":"Alert: Connection-Pool-Exhausted","to":"Component: HAProxy-LB","relationType":"monitors"}
{"from":"Incident: Primary-Node-Crash-2026-01-15","to":"Node: pg-primary-01","relationType":"affects"}
{"from":"Incident: Primary-Node-Crash-2026-01-15","to":"Node: pg-replica-02","relationType":"triggers_failover_to"}
{"from":"Incident: Network-Partition-2026-01-12","to":"Node: pg-primary-01","relationType":"affects"}
{"from":"Incident: Network-Partition-2026-01-12","to":"Component: Patroni-Cluster","relationType":"resolved_by"}
{"from":"Incident: Cascading-Failure-2026-01-08","to":"Cluster: PostgreSQL-HA-Prod","relationType":"affects"}
{"from":"CaseStudy: GitHub-MySQL-2012","to":"Cluster: PostgreSQL-HA-Prod","relationType":"informs"}
{"from":"CaseStudy: AWS-Aurora-Multi-AZ-2020","to":"Component: Patroni-Cluster","relationType":"informs"}
{"from":"CaseStudy: Netflix-Cassandra-Zone-Failure-2019","to":"Cluster: PostgreSQL-HA-Prod","relationType":"informs"}
{"from":"CaseStudy: Cloudflare-PostgreSQL-2023","to":"Component: etcd-cluster","relationType":"informs"}
{"from":"Incident: Primary-Disk-Failure-2026-01-15-02","to":"Node: pg-replica-02","relationType":"affects","createdBy":"Mai Th√†nh Duy An","createdAt":1768442500}
{"from":"Incident: Primary-Disk-Failure-2026-01-15-02","to":"Node: pg-replica-03","relationType":"triggers_failover_to","createdBy":"Mai Th√†nh Duy An","createdAt":1768442500}
